{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['40889 44022 10092 2471 9800 9561 38208 32528 3015 16920 16271 \\n10528 28193 16868 2572 11437 \\n33886 \\n38208 1200 \\n38208 13812 17231 3153 2710 2903 3015 16920 \\n40889 16972 2718 44022 43444 38478 2913 508 2718 32035 46265 \\n744 3455 9800 38208 1838 1944 2718 612 32528 3015 16920 9561 3025 13674 36506 21387 \\n28516 13247 40889 713 16920 26416 16868 3015 25634 \\n38208 1838 3429 1715 8775 32528 30081 709 21387 3015 16920 \\n24435 596 40889 13703 25312 18366 3015 13894 1330 18318 3082 596 30267 3015 24041 \\n3043 1259 40889 40526 35251 1200 \\n9561 38208 32528 16920 3015 16276 \\n44870 29696 38208 18412 24040 3015 30165 \\n33886 \\n38208 1200 \\n38208 3015 25958 17428 13812 596 1944 2718 612 1730 17231 3153 28654 1190 16970 21387 3015 16920 \\n', '31677 653 657 17998 1788 40889 24115 18366 22771 38208 \\n508 1259 1004 1985 10097 596 13703 25312 13894 1330 3015 30267 657 23542 31308 713 21413 14124 3015 47818 2913 38208 30055 3015 23840 13482 \\n732 1999 3068 24857 37150 40889 18366 \\n596 38478 36138 1521 \\n22771 1985 43965 \\n19233 9902 34332 \\n508 1259 596 27536 30081 1190 12739 1730 \\n33034 38208 16519 3015 2997 2875 1615 40889 18366 \\n40889 46922 24115 12928 3015 44782 27506 3413 2913 25607 1200 \\n596 24560 35916 1730 2345 3302 2471 597 33034 40889 18366 \\n20818 2500 657 25694 1788 40889 18366 1949 596 38208 \\n34802 13703 25312 18366 \\n']\n"
     ]
    }
   ],
   "source": [
    "# previous method\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), \"Document\", \"*\"))\n",
    "\n",
    "corpus_lexicon = []\n",
    "\n",
    "for file_path in file_list[0:2]:\n",
    "    with open(file_path) as f_input:\n",
    "        next(f_input)\n",
    "        next(f_input)\n",
    "        next(f_input)\n",
    "        lines = f_input.read()\n",
    "        lines = re.sub('-1', '', lines)\n",
    "        corpus_lexicon.append(lines)\n",
    "\n",
    "print(corpus_lexicon)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['40889', '44022', '10092', '2471', '9800', '9561', '38208', '32528', '3015', '16920', '16271', '10528', '28193', '16868', '2572', '11437', '33886', '38208', '1200', '38208', '13812', '17231', '3153', '2710', '2903', '3015', '16920', '40889', '16972', '2718', '44022', '43444', '38478', '2913', '508', '2718', '32035', '46265', '744', '3455', '9800', '38208', '1838', '1944', '2718', '612', '32528', '3015', '16920', '9561', '3025', '13674', '36506', '21387', '28516', '13247', '40889', '713', '16920', '26416', '16868', '3015', '25634', '38208', '1838', '3429', '1715', '8775', '32528', '30081', '709', '21387', '3015', '16920', '24435', '596', '40889', '13703', '25312', '18366', '3015', '13894', '1330', '18318', '3082', '596', '30267', '3015', '24041', '3043', '1259', '40889', '40526', '35251', '1200', '9561', '38208', '32528', '16920', '3015', '16276', '44870', '29696', '38208', '18412', '24040', '3015', '30165', '33886', '38208', '1200', '38208', '3015', '25958', '17428', '13812', '596', '1944', '2718', '612', '1730', '17231', '3153', '28654', '1190', '16970', '21387', '3015', '16920'], ['31677', '653', '657', '17998', '1788', '40889', '24115', '18366', '22771', '38208', '508', '1259', '1004', '1985', '10097', '596', '13703', '25312', '13894', '1330', '3015', '30267', '657', '23542', '31308', '713', '21413', '14124', '3015', '47818', '2913', '38208', '30055', '3015', '23840', '13482', '732', '1999', '3068', '24857', '37150', '40889', '18366', '596', '38478', '36138', '1521', '22771', '1985', '43965', '19233', '9902', '34332', '508', '1259', '596', '27536', '30081', '1190', '12739', '1730', '33034', '38208', '16519', '3015', '2997', '2875', '1615', '40889', '18366', '40889', '46922', '24115', '12928', '3015', '44782', '27506', '3413', '2913', '25607', '1200', '596', '24560', '35916', '1730', '2345', '3302', '2471', '597', '33034', '40889', '18366', '20818', '2500', '657', '25694', '1788', '40889', '18366', '1949', '596', '38208', '34802', '13703', '25312', '18366']]\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), \"Document\", \"*\"))\n",
    "\n",
    "corpusss = []\n",
    "count = 0\n",
    "file_count = 0\n",
    "\n",
    "for file_path in file_list[0:2]:\n",
    "    with open(file_path) as f_input:\n",
    "        next(f_input)\n",
    "        next(f_input)\n",
    "        next(f_input)\n",
    "        lines = f_input.read()\n",
    "        lines = re.sub('-1', '', lines)\n",
    "        lines = re.split(' |\\\\n', lines)\n",
    "        \n",
    "        for word in lines:\n",
    "            count += 1\n",
    "            if word == '':\n",
    "                lines.remove(word)\n",
    "        del lines[-1]\n",
    "        \n",
    "        corpusss.append(lines)\n",
    "        file_count += 1\n",
    "        \n",
    "print(corpusss)   \n",
    "print(count)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-264cd4fa4ed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mcorpusss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keys'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mfile_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'keys'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), \"Document\", \"*\"))\n",
    "\n",
    "corpusss = []\n",
    "\n",
    "#corpusss = []\n",
    "count = 0\n",
    "file_count = 0\n",
    "\n",
    "for file_path in file_list[0:2]:\n",
    "    with open(file_path) as f_input:\n",
    "        next(f_input)\n",
    "        next(f_input)\n",
    "        next(f_input)\n",
    "        lines = f_input.read()\n",
    "        lines = re.sub('-1', '', lines)\n",
    "        lines = re.split(' |\\\\n', lines)\n",
    "        \n",
    "        for word in lines:\n",
    "            count += 1\n",
    "            if word == '':\n",
    "                lines.remove(word)\n",
    "        del lines[-1]\n",
    "        \n",
    "        corpusss[].append(lines)\n",
    "        file_count += 1\n",
    "        \n",
    "print(corpusss)  \n",
    "print(file_count)\n",
    "print(count)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "    \n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "\n",
    "\n",
    "d = {'keys', 'values'}\n",
    "\n",
    "\n",
    "\n",
    "for doc in corpusss:\n",
    "    #tf2 = Counter()\n",
    "    for word in doc:\n",
    "        tf2[word] += 1\n",
    "    #print(tf2.items())\n",
    "    d = dict(Counter(tf2.items()))\n",
    "\n",
    "print(d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous method\n",
    "\n",
    "def build_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "vocab = build_lexicon(corpus_lexicon)\n",
    "\n",
    "print('vocabulary')\n",
    "print(vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous method\n",
    "\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "doc_term_matrix = []\n",
    "for doc in corpus_lexicon:\n",
    "    print('The doc is ')\n",
    "    print(doc)\n",
    "    tf_vector = [tf(word, doc) for word in vocab]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd')for freq in tf_vector)\n",
    "    print('the tf vector for Document %d is [%s]'%((corpus_lexicon.index(doc)+1), tf_vector_string))\n",
    "    print('\\n')\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "    \n",
    "print('All Combined here is our master document term matrix: ')\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous method\n",
    "\n",
    "def l2_normalizer(vec):\n",
    "    denom = sum([el**2 for el in vec])\n",
    "    return[(el/math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2 = []\n",
    "\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "    \n",
    "print('A regular old document term matrix: ')\n",
    "#print(np.matrix(doc_term_matrix))\n",
    "print(doc_term_matrix)\n",
    "print(len(doc_term_matrix))\n",
    "\n",
    "print('\\n A document term matrix with row-wise L2 norms of 1: ')\n",
    "#print(np.matrix(doc_term_matrix_l2))\n",
    "print(np.matrix(doc_term_matrix_l2))\n",
    "print(len(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous method\n",
    "\n",
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount += 1\n",
    "    return doccount\n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / 1+df)\n",
    "\n",
    "for word in vocab:\n",
    "    df_count = numDocsContaining(word, corpus_lexicon)\n",
    "print(df_count)\n",
    "\n",
    "my_idf_vector = [idf(word, corpus_lexicon) for word in vocab]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Our vocabulary vector is [' + ', '.join(list(vocab)) + ']')\n",
    "print('length of vocab')\n",
    "print(len(list(vocab)))\n",
    "print('\\n')\n",
    "\n",
    "print('\\n A document term matrix with row-wise L2 norms of 1: ')\n",
    "#print(np.matrix(doc_term_matrix_l2))\n",
    "print(np.matrix(doc_term_matrix_l2))\n",
    "\n",
    "print('The inverse document frequency vector is ')\n",
    "print(np.matrix(my_idf_vector))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous method\n",
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "\n",
    "print(my_idf_matrix)\n",
    "print(len(my_idf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous method\n",
    "doc_term_matrix_tfidf = []\n",
    "\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "    df_count += 1\n",
    "    print(df_count)\n",
    "    \n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "df_count = 0\n",
    "\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "    \n",
    "    \n",
    "print(vocab)\n",
    "print(np.matrix(doc_term_matrix_tfidf_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query part here on, previous method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"Query\", \"*\"))\n",
    "\n",
    "query_corpus = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    with open(file_path) as f_input:\n",
    "        lines = f_input.read()\n",
    "        lines = re.sub('-1', '', lines)\n",
    "        lines = lines.strip('\\n')\n",
    "        query_corpus.append(lines)\n",
    "\n",
    "print(query_corpus)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_query = build_lexicon(query_corpus)\n",
    "\n",
    "doc_term_matrix_query = []\n",
    "print('Our query vocabulary vector is [' + ', '.join(list(vocab)) + ']')\n",
    "\n",
    "for query in query_corpus:\n",
    "    print('The query is ')\n",
    "    print(query)\n",
    "    tf_query_vector = [tf(word, doc) for word in vocabulary_query]\n",
    "    tf_query_vector_string = ', '.join(format(freq, 'd')for freq in tf_query_vector)\n",
    "    print('the tf_query vector for Query %d is [%s]'%((query_corpus.index(query)+1), tf_query_vector_string))\n",
    "    doc_term_matrix_query.append(tf_query_vector)\n",
    "    \n",
    "print('All Combined here is our master query term matrix: ')\n",
    "print(doc_term_matrix_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix_query_l2 = []\n",
    "\n",
    "for vec in doc_term_matrix_query:\n",
    "    doc_term_matrix_query_l2.append(l2_normalizer(vec))\n",
    "    \n",
    "print('A regular old query term matrix: ')\n",
    "print(np.matrix(doc_term_matrix_query))\n",
    "print('\\n A query term matrix with row-wise L2 norms of 1: ')\n",
    "print(np.matrix(doc_term_matrix_query_l2))\n",
    "print('here is the length')\n",
    "print(len(doc_term_matrix_query_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix_query_tfidf = []\n",
    "\n",
    "\n",
    "for tf_query_vector in doc_term_matrix_query:\n",
    "    #doc_term_matrix_query_tfidf.append(np.dot(tf_query_vector, my_idf_vector))\n",
    "    doc_term_matrix_query_tfidf.append(tf_query_vector * my_idf_vector)\n",
    "    #np.dot(np.ones((97,2)), np.ones((2,1)))\n",
    "\n",
    "    #doc_term_matrix_query_tfidf.append(doc_term_matrix_query_l2*my_idf_matrix)\n",
    "doc_term_matrix_query_tfidf_l2 = []\n",
    "for tf_query_vector in doc_term_matrix_query_tfidf:\n",
    "    doc_term_matrix_query_tfidf_l2.append(l2_normalizer(tf_query_vector))\n",
    "    \n",
    "print(vocabulary_query)\n",
    "print(np.matrix(doc_term_matrix_query_tfidf_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "    \n",
    "def freq(term, document):\n",
    "    for term in document:\n",
    "        tf = Counter()\n",
    "        for word in term:\n",
    "            tf[word] += 1\n",
    "    d_tf = dict(Counter(tf))\n",
    "    return d_tf\n",
    "\n",
    "print('Our vocabulary vector is: \\n')\n",
    "vocabulary = d.keys()\n",
    "print(vocabulary)\n",
    "for doc in corpusss:\n",
    "    print('\\n')\n",
    "    print('The doc is  ')\n",
    "    print(doc)\n",
    "    \n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd')for freq in tf_vector)\n",
    "    print('the tf vector for Document %d is [%s]'%((corpusss.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
