{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['40889 44022 10092 2471 9800 9561 38208 32528 3015 16920 16271 \\n10528 28193 16868 2572 11437 \\n33886 \\n38208 1200 \\n38208 13812 17231 3153 2710 2903 3015 16920 \\n40889 16972 2718 44022 43444 38478 2913 508 2718 32035 46265 \\n744 3455 9800 38208 1838 1944 2718 612 32528 3015 16920 9561 3025 13674 36506 21387 \\n28516 13247 40889 713 16920 26416 16868 3015 25634 \\n38208 1838 3429 1715 8775 32528 30081 709 21387 3015 16920 \\n24435 596 40889 13703 25312 18366 3015 13894 1330 18318 3082 596 30267 3015 24041 \\n3043 1259 40889 40526 35251 1200 \\n9561 38208 32528 16920 3015 16276 \\n44870 29696 38208 18412 24040 3015 30165 \\n33886 \\n38208 1200 \\n38208 3015 25958 17428 13812 596 1944 2718 612 1730 17231 3153 28654 1190 16970 21387 3015 16920 \\n', '31677 653 657 17998 1788 40889 24115 18366 22771 38208 \\n508 1259 1004 1985 10097 596 13703 25312 13894 1330 3015 30267 657 23542 31308 713 21413 14124 3015 47818 2913 38208 30055 3015 23840 13482 \\n732 1999 3068 24857 37150 40889 18366 \\n596 38478 36138 1521 \\n22771 1985 43965 \\n19233 9902 34332 \\n508 1259 596 27536 30081 1190 12739 1730 \\n33034 38208 16519 3015 2997 2875 1615 40889 18366 \\n40889 46922 24115 12928 3015 44782 27506 3413 2913 25607 1200 \\n596 24560 35916 1730 2345 3302 2471 597 33034 40889 18366 \\n20818 2500 657 25694 1788 40889 18366 1949 596 38208 \\n34802 13703 25312 18366 \\n']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), \"Document\", \"*\"))\n",
    "\n",
    "corpusss = []\n",
    "\n",
    "for file_path in file_list[0:2]:\n",
    "    with open(file_path) as f_input:\n",
    "        next(f_input)\n",
    "        next(f_input)\n",
    "        next(f_input)\n",
    "        lines = f_input.read()\n",
    "        lines = re.sub('-1', '', lines)\n",
    "        corpusss.append(lines)\n",
    "\n",
    "print(corpusss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-9b50b81688a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for doc in corpus:\n",
    "    tf2 = Counter()\n",
    "    for word in doc.split():\n",
    "        tf2[word] += 1\n",
    "    print(tf2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('40889', 5), ('44022', 2), ('10092', 1), ('2471', 1), ('9800', 2), ('9561', 3), ('38208', 9), ('32528', 4), ('3015', 11), ('16920', 7), ('16271', 1), ('10528', 1), ('28193', 1), ('16868', 2), ('2572', 1), ('11437', 1), ('33886', 2), ('1200', 3), ('13812', 2), ('17231', 2), ('3153', 2), ('2710', 1), ('2903', 1), ('16972', 1), ('2718', 4), ('43444', 1), ('38478', 1), ('2913', 1), ('508', 1), ('32035', 1), ('46265', 1), ('744', 1), ('3455', 1), ('1838', 2), ('1944', 2), ('612', 2), ('3025', 1), ('13674', 1), ('36506', 1), ('21387', 3), ('28516', 1), ('13247', 1), ('713', 1), ('26416', 1), ('25634', 1), ('3429', 1), ('1715', 1), ('8775', 1), ('30081', 1), ('709', 1), ('24435', 1), ('596', 3), ('13703', 1), ('25312', 1), ('18366', 1), ('13894', 1), ('1330', 1), ('18318', 1), ('3082', 1), ('30267', 1), ('24041', 1), ('3043', 1), ('1259', 1), ('40526', 1), ('35251', 1), ('16276', 1), ('44870', 1), ('29696', 1), ('18412', 1), ('24040', 1), ('30165', 1), ('25958', 1), ('17428', 1), ('1730', 1), ('28654', 1), ('1190', 1), ('16970', 1)])\n",
      "dict_items([('31677', 1), ('653', 1), ('657', 3), ('17998', 1), ('1788', 2), ('40889', 6), ('24115', 2), ('18366', 6), ('22771', 2), ('38208', 4), ('508', 2), ('1259', 2), ('1004', 1), ('1985', 2), ('10097', 1), ('596', 5), ('13703', 2), ('25312', 2), ('13894', 1), ('1330', 1), ('3015', 5), ('30267', 1), ('23542', 1), ('31308', 1), ('713', 1), ('21413', 1), ('14124', 1), ('47818', 1), ('2913', 2), ('30055', 1), ('23840', 1), ('13482', 1), ('732', 1), ('1999', 1), ('3068', 1), ('24857', 1), ('37150', 1), ('38478', 1), ('36138', 1), ('1521', 1), ('43965', 1), ('19233', 1), ('9902', 1), ('34332', 1), ('27536', 1), ('30081', 1), ('1190', 1), ('12739', 1), ('1730', 2), ('33034', 2), ('16519', 1), ('2997', 1), ('2875', 1), ('1615', 1), ('46922', 1), ('12928', 1), ('44782', 1), ('27506', 1), ('3413', 1), ('25607', 1), ('1200', 1), ('24560', 1), ('35916', 1), ('2345', 1), ('3302', 1), ('2471', 1), ('597', 1), ('20818', 1), ('2500', 1), ('25694', 1), ('1949', 1), ('34802', 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "for doc in corpusss:\n",
    "    tf = Counter()\n",
    "    for word in re.split(' |\\\\n', doc):\n",
    "        tf[word] += 1\n",
    "    del tf['']\n",
    "    print(tf.items())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [36506, 36138, 44870, 27506, 46922, 744, 30055, 2718, 13247, 657, 508, 16868, 1521, 1200, 1999, 9561, 16920, 2875, 24857, 34332, 16276, 10092, 10097, 12928, 597, 12739, 25634, 25312, 38208, 38478, 28516, 21387, 1615, 1788, 47818, 3082, 13894, 2903, 18318, 1259, 30267, 32528, 26416, 1985, 30081, 1715, 21413, 16970, 3043, 32035, 3455, 43965, 24435, 16519, 46265, 31308, 25958, 24040, 13812, 1949, 3153, 2572, 40526, 13674, 25694, 24115, 43444, 24041, 27536, 44782, 31677, 18366, 3025, 24560, 1838, 16972, 3015, 35916, 17428, 13703, 3302, 3429, 1330, 612, 20818, 709, 11437, 9800, 33034, 732, 3068, 2710, 9902, 28193, 22771, 25607, 34802, 13482, 30165, 23840, 2471, 2500, 1004, 2913, 14124, 17231, 19233, 596, 713, 2345, 3413, 17998, 33886, 2997, 37150, 23542, 1944, 35251, 40889, 18412, 29696, 10528, 16271, 8775, 28654, 1190, 653, 1730, 44022]\n",
      "The doc is \"40889 44022 10092 2471 9800 9561 38208 32528 3015 16920 16271 \n",
      "10528 28193 16868 2572 11437 \n",
      "33886 \n",
      "38208 1200 \n",
      "38208 13812 17231 3153 2710 2903 3015 16920 \n",
      "40889 16972 2718 44022 43444 38478 2913 508 2718 32035 46265 \n",
      "744 3455 9800 38208 1838 1944 2718 612 32528 3015 16920 9561 3025 13674 36506 21387 \n",
      "28516 13247 40889 713 16920 26416 16868 3015 25634 \n",
      "38208 1838 3429 1715 8775 32528 30081 709 21387 3015 16920 \n",
      "24435 596 40889 13703 25312 18366 3015 13894 1330 18318 3082 596 30267 3015 24041 \n",
      "3043 1259 40889 40526 35251 1200 \n",
      "9561 38208 32528 16920 3015 16276 \n",
      "44870 29696 38208 18412 24040 3015 30165 \n",
      "33886 \n",
      "38208 1200 \n",
      "38208 3015 25958 17428 13812 596 1944 2718 612 1730 17231 3153 28654 1190 16970 21387 3015 16920 \n",
      "\" \n",
      "the tf vector for Document 1 is [1, 0, 1, 0, 0, 1, 0, 4, 1, 0, 1, 2, 0, 3, 0, 3, 7, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 9, 1, 1, 3, 0, 0, 0, 1, 1, 1, 1, 1, 1, 4, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 11, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 3, 1, 0, 0, 0, 2, 0, 0, 0, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2]\n",
      "The doc is \"31677 653 657 17998 1788 40889 24115 18366 22771 38208 \n",
      "508 1259 1004 1985 10097 596 13703 25312 13894 1330 3015 30267 657 23542 31308 713 21413 14124 3015 47818 2913 38208 30055 3015 23840 13482 \n",
      "732 1999 3068 24857 37150 40889 18366 \n",
      "596 38478 36138 1521 \n",
      "22771 1985 43965 \n",
      "19233 9902 34332 \n",
      "508 1259 596 27536 30081 1190 12739 1730 \n",
      "33034 38208 16519 3015 2997 2875 1615 40889 18366 \n",
      "40889 46922 24115 12928 3015 44782 27506 3413 2913 25607 1200 \n",
      "596 24560 35916 1730 2345 3302 2471 597 33034 40889 18366 \n",
      "20818 2500 657 25694 1788 40889 18366 1949 596 38208 \n",
      "34802 13703 25312 18366 \n",
      "\" \n",
      "the tf vector for Document 2 is [0, 1, 0, 1, 1, 0, 1, 0, 0, 3, 2, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 4, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 6, 0, 1, 0, 0, 5, 1, 0, 2, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 5, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 6, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0]\n",
      "All Combined here is our master document term matrix: \n",
      "[[1, 0, 1, 0, 0, 1, 0, 4, 1, 0, 1, 2, 0, 3, 0, 3, 7, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 9, 1, 1, 3, 0, 0, 0, 1, 1, 1, 1, 1, 1, 4, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 11, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 3, 1, 0, 0, 0, 2, 0, 0, 0, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2], [0, 1, 0, 1, 1, 0, 1, 0, 0, 3, 2, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 4, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 6, 0, 1, 0, 0, 5, 1, 0, 2, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 5, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 6, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "def build_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "vocabulary = build_lexicon(corpusss)\n",
    "\n",
    "doc_term_matrix = []\n",
    "print('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "\n",
    "for doc in corpusss:\n",
    "    print('The doc is \"' + doc + '\" ')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd')for freq in tf_vector)\n",
    "    print('the tf vector for Document %d is [%s]'%((corpusss.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "    \n",
    "print('All Combined here is our master document term matrix: ')\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regular old document term matrix: \n",
      "[[1, 0, 1, 0, 0, 1, 0, 4, 1, 0, 1, 2, 0, 3, 0, 3, 7, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 9, 1, 1, 3, 0, 0, 0, 1, 1, 1, 1, 1, 1, 4, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 2, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 11, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 3, 1, 0, 0, 0, 2, 0, 0, 0, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2], [0, 1, 0, 1, 1, 0, 1, 0, 0, 3, 2, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 4, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 6, 0, 1, 0, 0, 5, 1, 0, 2, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 5, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 6, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0]]\n",
      "2\n",
      "\n",
      " A document term matrix with row-wise L2 norms of 1: \n",
      "[[ 0.04761905  0.          0.04761905  0.          0.          0.04761905\n",
      "   0.          0.19047619  0.04761905  0.          0.04761905  0.0952381\n",
      "   0.          0.14285714  0.          0.14285714  0.33333333  0.          0.\n",
      "   0.          0.04761905  0.04761905  0.          0.          0.          0.\n",
      "   0.04761905  0.04761905  0.42857143  0.04761905  0.04761905  0.14285714\n",
      "   0.          0.          0.          0.04761905  0.04761905  0.04761905\n",
      "   0.04761905  0.04761905  0.04761905  0.19047619  0.04761905  0.\n",
      "   0.04761905  0.04761905  0.          0.04761905  0.04761905  0.04761905\n",
      "   0.04761905  0.          0.04761905  0.          0.04761905  0.\n",
      "   0.04761905  0.04761905  0.0952381   0.          0.0952381   0.04761905\n",
      "   0.04761905  0.04761905  0.          0.          0.04761905  0.04761905\n",
      "   0.          0.          0.          0.04761905  0.04761905  0.\n",
      "   0.0952381   0.04761905  0.52380952  0.          0.04761905  0.04761905\n",
      "   0.          0.04761905  0.04761905  0.0952381   0.          0.04761905\n",
      "   0.04761905  0.0952381   0.          0.          0.          0.04761905\n",
      "   0.          0.04761905  0.          0.          0.          0.\n",
      "   0.04761905  0.          0.04761905  0.          0.          0.04761905\n",
      "   0.          0.0952381   0.          0.14285714  0.04761905  0.          0.\n",
      "   0.          0.0952381   0.          0.          0.          0.0952381\n",
      "   0.04761905  0.23809524  0.04761905  0.04761905  0.04761905  0.04761905\n",
      "   0.04761905  0.04761905  0.04761905  0.          0.04761905  0.0952381 ]\n",
      " [ 0.          0.06375767  0.          0.06375767  0.06375767  0.\n",
      "   0.06375767  0.          0.          0.19127301  0.12751534  0.\n",
      "   0.06375767  0.06375767  0.06375767  0.          0.          0.06375767\n",
      "   0.06375767  0.06375767  0.          0.          0.06375767  0.06375767\n",
      "   0.06375767  0.06375767  0.          0.12751534  0.25503069  0.06375767\n",
      "   0.          0.          0.06375767  0.12751534  0.06375767  0.\n",
      "   0.06375767  0.          0.          0.12751534  0.06375767  0.          0.\n",
      "   0.12751534  0.06375767  0.          0.06375767  0.          0.          0.\n",
      "   0.          0.06375767  0.          0.06375767  0.          0.06375767\n",
      "   0.          0.          0.          0.06375767  0.          0.          0.\n",
      "   0.          0.06375767  0.12751534  0.          0.          0.06375767\n",
      "   0.06375767  0.06375767  0.38254603  0.          0.06375767  0.          0.\n",
      "   0.31878836  0.06375767  0.          0.12751534  0.06375767  0.\n",
      "   0.06375767  0.          0.06375767  0.          0.          0.\n",
      "   0.12751534  0.06375767  0.06375767  0.          0.06375767  0.\n",
      "   0.12751534  0.06375767  0.06375767  0.06375767  0.          0.06375767\n",
      "   0.06375767  0.06375767  0.06375767  0.12751534  0.06375767  0.\n",
      "   0.06375767  0.31878836  0.06375767  0.06375767  0.06375767  0.06375767\n",
      "   0.          0.06375767  0.06375767  0.06375767  0.          0.\n",
      "   0.38254603  0.          0.          0.          0.          0.          0.\n",
      "   0.06375767  0.06375767  0.12751534  0.        ]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def l2_normalizer(vec):\n",
    "    denom = sum([el**2 for el in vec])\n",
    "    return[(el/math.sqrt(denom)) for el in vec]\n",
    "\n",
    "doc_term_matrix_l2 = []\n",
    "\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "    \n",
    "print('A regular old document term matrix: ')\n",
    "#print(np.matrix(doc_term_matrix))\n",
    "print(doc_term_matrix)\n",
    "print(len(doc_term_matrix))\n",
    "\n",
    "print('\\n A document term matrix with row-wise L2 norms of 1: ')\n",
    "#print(np.matrix(doc_term_matrix_l2))\n",
    "print(np.matrix(doc_term_matrix_l2))\n",
    "print(len(doc_term_matrix_l2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.38629436  1.09861229\n",
      "   1.09861229  1.38629436  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.38629436  1.38629436  1.38629436\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.38629436  1.09861229  1.09861229  1.38629436  1.38629436  1.09861229\n",
      "   1.09861229  1.09861229  1.38629436  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.38629436\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.38629436  1.09861229\n",
      "   1.09861229  1.38629436  1.09861229  1.09861229  1.38629436  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.38629436  1.09861229\n",
      "   1.09861229  1.38629436  1.09861229  1.09861229  1.09861229  1.38629436\n",
      "   1.38629436  1.09861229  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.38629436  1.09861229\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  1.09861229  1.38629436\n",
      "   1.09861229  1.38629436  1.09861229]]\n",
      "129\n",
      "\n",
      "\n",
      "Our vocabulary vector is [36506, 36138, 44870, 27506, 46922, 744, 30055, 2718, 13247, 657, 508, 16868, 1521, 1200, 1999, 9561, 16920, 2875, 24857, 34332, 16276, 10092, 10097, 12928, 597, 12739, 25634, 25312, 38208, 38478, 28516, 21387, 1615, 1788, 47818, 3082, 13894, 2903, 18318, 1259, 30267, 32528, 26416, 1985, 30081, 1715, 21413, 16970, 3043, 32035, 3455, 43965, 24435, 16519, 46265, 31308, 25958, 24040, 13812, 1949, 3153, 2572, 40526, 13674, 25694, 24115, 43444, 24041, 27536, 44782, 31677, 18366, 3025, 24560, 1838, 16972, 3015, 35916, 17428, 13703, 3302, 3429, 1330, 612, 20818, 709, 11437, 9800, 33034, 732, 3068, 2710, 9902, 28193, 22771, 25607, 34802, 13482, 30165, 23840, 2471, 2500, 1004, 2913, 14124, 17231, 19233, 596, 713, 2345, 3413, 17998, 33886, 2997, 37150, 23542, 1944, 35251, 40889, 18412, 29696, 10528, 16271, 8775, 28654, 1190, 653, 1730, 44022]\n",
      "length of vocab\n",
      "129\n",
      "\n",
      "\n",
      "The inverse document frequency vector is [1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.386294, 1.386294, 1.386294, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.386294, 1.386294, 1.098612, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.098612, 1.386294, 1.386294, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.386294, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.098612, 1.386294, 1.098612, 1.386294, 1.098612]\n"
     ]
    }
   ],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount += 1\n",
    "    return doccount\n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / 1+df)\n",
    "\n",
    "my_idf_vector = [idf(word, corpusss) for word in vocabulary]\n",
    "\n",
    "\n",
    "print(np.matrix(my_idf_vector))\n",
    "print(len(my_idf_vector))\n",
    "print('\\n')\n",
    "\n",
    "print('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "print('length of vocab')\n",
    "print(len(list(vocabulary)))\n",
    "print('\\n')\n",
    "\n",
    "print('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.09861229  0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          1.09861229  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          1.09861229 ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  1.09861229  0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          1.38629436  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          1.09861229]]\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "\n",
    "print(my_idf_matrix)\n",
    "print(len(my_idf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'36506', '36138', '44870', '27506', '46922', '744', '30055', '2718', '13247', '657', '508', '16868', '1521', '1200', '1999', '9561', '16920', '2875', '24857', '34332', '16276', '10092', '10097', '12928', '597', '12739', '25634', '25312', '38208', '38478', '28516', '21387', '1615', '1788', '47818', '3082', '13894', '2903', '18318', '1259', '30267', '32528', '26416', '1985', '30081', '1715', '21413', '16970', '3043', '32035', '3455', '43965', '24435', '16519', '46265', '31308', '25958', '24040', '13812', '1949', '3153', '2572', '40526', '13674', '25694', '24115', '43444', '24041', '27536', '44782', '31677', '18366', '3025', '24560', '1838', '16972', '3015', '35916', '17428', '13703', '3302', '3429', '1330', '612', '20818', '709', '11437', '9800', '33034', '732', '3068', '2710', '9902', '28193', '22771', '25607', '34802', '13482', '30165', '23840', '2471', '2500', '1004', '2913', '14124', '17231', '19233', '596', '713', '2345', '3413', '17998', '33886', '2997', '37150', '23542', '1944', '35251', '40889', '18412', '29696', '10528', '16271', '8775', '28654', '1190', '653', '1730', '44022'}\n",
      "[[ 0.04099617  0.          0.04099617  0.          0.          0.04099617\n",
      "   0.          0.16398467  0.04099617  0.          0.0517314   0.08199233\n",
      "   0.          0.15519421  0.          0.1229885   0.28697317  0.          0.\n",
      "   0.          0.04099617  0.04099617  0.          0.          0.          0.\n",
      "   0.04099617  0.0517314   0.46558263  0.0517314   0.04099617  0.1229885\n",
      "   0.          0.          0.          0.04099617  0.0517314   0.04099617\n",
      "   0.04099617  0.0517314   0.0517314   0.16398467  0.04099617  0.\n",
      "   0.0517314   0.04099617  0.          0.04099617  0.04099617  0.04099617\n",
      "   0.04099617  0.          0.04099617  0.          0.04099617  0.\n",
      "   0.04099617  0.04099617  0.08199233  0.          0.08199233  0.04099617\n",
      "   0.04099617  0.04099617  0.          0.          0.04099617  0.04099617\n",
      "   0.          0.          0.          0.0517314   0.04099617  0.\n",
      "   0.08199233  0.04099617  0.56904543  0.          0.04099617  0.0517314\n",
      "   0.          0.04099617  0.0517314   0.08199233  0.          0.04099617\n",
      "   0.04099617  0.08199233  0.          0.          0.          0.04099617\n",
      "   0.          0.04099617  0.          0.          0.          0.\n",
      "   0.04099617  0.          0.0517314   0.          0.          0.0517314\n",
      "   0.          0.08199233  0.          0.15519421  0.0517314   0.          0.\n",
      "   0.          0.08199233  0.          0.          0.          0.08199233\n",
      "   0.04099617  0.25865701  0.04099617  0.04099617  0.04099617  0.04099617\n",
      "   0.04099617  0.04099617  0.0517314   0.          0.0517314   0.08199233]\n",
      " [ 0.          0.05366105  0.          0.05366105  0.05366105  0.\n",
      "   0.05366105  0.          0.          0.16098316  0.13542542  0.\n",
      "   0.05366105  0.06771271  0.05366105  0.          0.          0.05366105\n",
      "   0.05366105  0.05366105  0.          0.          0.05366105  0.05366105\n",
      "   0.05366105  0.05366105  0.          0.13542542  0.27085083  0.06771271\n",
      "   0.          0.          0.05366105  0.1073221   0.05366105  0.\n",
      "   0.06771271  0.          0.          0.13542542  0.06771271  0.          0.\n",
      "   0.1073221   0.06771271  0.          0.05366105  0.          0.          0.\n",
      "   0.          0.05366105  0.          0.05366105  0.          0.05366105\n",
      "   0.          0.          0.          0.05366105  0.          0.          0.\n",
      "   0.          0.05366105  0.1073221   0.          0.          0.05366105\n",
      "   0.05366105  0.05366105  0.40627625  0.          0.05366105  0.          0.\n",
      "   0.33856354  0.05366105  0.          0.13542542  0.05366105  0.\n",
      "   0.06771271  0.          0.05366105  0.          0.          0.\n",
      "   0.1073221   0.05366105  0.05366105  0.          0.05366105  0.\n",
      "   0.1073221   0.05366105  0.05366105  0.05366105  0.          0.05366105\n",
      "   0.06771271  0.05366105  0.05366105  0.13542542  0.05366105  0.\n",
      "   0.05366105  0.33856354  0.06771271  0.05366105  0.05366105  0.05366105\n",
      "   0.          0.05366105  0.05366105  0.05366105  0.          0.\n",
      "   0.40627625  0.          0.          0.          0.          0.          0.\n",
      "   0.06771271  0.05366105  0.13542542  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_term_matrix_tfidf = []\n",
    "\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "\n",
    "    \n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "    \n",
    "print(vocabulary)\n",
    "print(np.matrix(doc_term_matrix_tfidf_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "tfidf_matrix_doc = tfidf_vectorizer.fit_transform(corpusss)\n",
    "\n",
    "print(tfidf_matrix_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2280 3068 457 26763 27631 14645 732 4332 2690 2923 20646 38527 27352 \\n612 \\n1407 2986 \\n25607 432 2148 3140 \\n27414 31424 7680 2280 3068 457 24554 30066 855 1715 11629 \\n26763 18571 27631 14645 732 4332 \\n28226 23520 27646 21192 3015 24310 23142 \\n2280 3068 457 1200 \\n596 855 3015 732 1715 2033 \\n27414 2345 2471 699 24310 43624 2878 32518 13629 17554 \\n17030 18380 9561 \\n31765 19448 31359 3012 18828 \\n27646 2345 21192 11738 3015 23600 \\n596 23600 21189 341 12133 3015 1259 18571 27631 3015 14645 \\n7680 2572 8994 2345 21892 3123 2855 \\n8250 32818 699 24310 7971 30165 9608 3015 23142 \\n2280 3068 457 1200 \\n27414 3015 14177 469 1259 732 3237 24583 14177 \\n34043 9079 24310 2938 13109 34474 \\n20818 3015 13629 34445 13910 \\n24310 1408 34770 379 28672 29978 3015 19217 \\n21478 16574 10340 28940 \\n24054 3015 32474 1906 739 34445 2737 \\n9242 2938 1259 10355 3015 15128 \\n1155 34179 13023 \\n8769 17442 2717 24783 14645 19486 \\n3025 3379 39506 \\n27414 2345 1728 34975 662 27647 25995 25944 7570 23390 24310 3015 7971 \\n1027 ', '44394 9715 29223 5580 982 38527 41840 \\n612 \\n1407 2986 21413 7680 44394 \\n1407 9715 29223 1077 36874 \\n1167 3015 1521 35066 1682 16840 28021 1774 3844 2184 \\n3748 985 3981 2446 657 2620 3302 9058 3015 840 27960 \\n739 29223 1155 567 11408 3748 985 3981 2446 2231 32194 18057 \\n44394 596 35066 2913 25607 1200 \\n9242 8805 1259 20417 3015 \\n973 32138 21442 11408 39902 2338 3148 16737 11037 3015 19572 16739 \\n44394 35547 596 24806 21413 30868 2572 42911 25607 33687 1278 \\n732 597 29223 1077 3748 985 3981 2446 657 3302 9058 27960 \\n3318 34179 2471 20054 10369 29762 2913 32194 3015 30802 \\n29013 30802 35034 16743 3015 41573 3981 1152 3596 18296 1200 \\n1521 35066 28421 18366 1975 3073 \\n15404 1565 2715 1155 2421 1656 40732 \\n20568 1259 3748 985 3981 2446 25066 1153 1077 44394 3015 24970 2428 44394 11408 1153 2231 32194 332 14084 3015 20568 \\n1153 1200 \\n9242 32253 1259 1153 1077 3748 985 3981 2446 15546 1278 14817 1920 926 3015 \\n3748 985 3981 2446 \\n1715 \\n612 23728 596 35066 28421 \\n732 1715 3429 2535 22771 \\n44394 29218 947 2091 2033 \\n1407 739 2913 25607 1200 \\n1153 13023 9242 8805 23908 1259 20417 3015 \\n947 2091 2033 2500 1200 \\n41382 \\n44394 33905 16464 35523 8976 1759 3015 2572 31401 22656 3015 28417 \\n9242 28417 2913 27646 21442 18352 27282 \\n21413 2625 2471 30218 10944 2913 32194 29835 24523 \\n732 870 2625 26147 624 18329 \\n906 596 2913 508 732 16743 38731 1004 16893 582 \\n1027 ']\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"Query\", \"*\"))\n",
    "\n",
    "query_corpus = []\n",
    "\n",
    "for file_path in file_list[0:2]:\n",
    "    with open(file_path) as f_input:\n",
    "        lines = f_input.read()\n",
    "        lines = re.sub('-1', '', lines)\n",
    "        lines = lines.strip('\\n')\n",
    "        query_corpus.append(lines)\n",
    "\n",
    "print(query_corpus)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('2280', 4), ('3068', 4), ('457', 4), ('26763', 2), ('27631', 3), ('14645', 4), ('732', 4), ('4332', 2), ('2690', 1), ('2923', 1), ('20646', 1), ('38527', 1), ('27352', 1), ('612', 1), ('1407', 1), ('2986', 1), ('25607', 1), ('432', 1), ('2148', 1), ('3140', 1), ('27414', 4), ('31424', 1), ('7680', 2), ('24554', 1), ('30066', 1), ('855', 2), ('1715', 2), ('11629', 1), ('18571', 2), ('28226', 1), ('23520', 1), ('27646', 2), ('21192', 2), ('3015', 12), ('24310', 6), ('23142', 2), ('1200', 2), ('596', 2), ('2033', 1), ('2345', 4), ('2471', 1), ('699', 2), ('43624', 1), ('2878', 1), ('32518', 1), ('13629', 2), ('17554', 1), ('17030', 1), ('18380', 1), ('9561', 1), ('31765', 1), ('19448', 1), ('31359', 1), ('3012', 1), ('18828', 1), ('11738', 1), ('23600', 2), ('21189', 1), ('341', 1), ('12133', 1), ('1259', 3), ('2572', 1), ('8994', 1), ('21892', 1), ('3123', 1), ('2855', 1), ('8250', 1), ('32818', 1), ('7971', 2), ('30165', 1), ('9608', 1), ('14177', 2), ('469', 1), ('3237', 1), ('24583', 1), ('34043', 1), ('9079', 1), ('2938', 2), ('13109', 1), ('34474', 1), ('20818', 1), ('34445', 2), ('13910', 1), ('1408', 1), ('34770', 1), ('379', 1), ('28672', 1), ('29978', 1), ('19217', 1), ('21478', 1), ('16574', 1), ('10340', 1), ('28940', 1), ('24054', 1), ('32474', 1), ('1906', 1), ('739', 1), ('2737', 1), ('9242', 1), ('10355', 1), ('15128', 1), ('1155', 1), ('34179', 1), ('13023', 1), ('8769', 1), ('17442', 1), ('2717', 1), ('24783', 1), ('19486', 1), ('3025', 1), ('3379', 1), ('39506', 1), ('1728', 1), ('34975', 1), ('662', 1), ('27647', 1), ('25995', 1), ('25944', 1), ('7570', 1), ('23390', 1), ('1027', 1)])\n",
      "dict_items([('44394', 8), ('9715', 2), ('29223', 4), ('5580', 1), ('982', 1), ('38527', 1), ('41840', 1), ('612', 2), ('1407', 3), ('2986', 1), ('21413', 3), ('7680', 1), ('1077', 4), ('36874', 1), ('1167', 1), ('3015', 12), ('1521', 2), ('35066', 4), ('1682', 1), ('16840', 1), ('28021', 1), ('1774', 1), ('3844', 1), ('2184', 1), ('3748', 6), ('985', 6), ('3981', 7), ('2446', 6), ('657', 2), ('2620', 1), ('3302', 2), ('9058', 2), ('840', 1), ('27960', 2), ('739', 2), ('1155', 2), ('567', 1), ('11408', 3), ('2231', 2), ('32194', 4), ('18057', 1), ('596', 4), ('2913', 6), ('25607', 3), ('1200', 5), ('9242', 4), ('8805', 2), ('1259', 4), ('20417', 2), ('973', 1), ('32138', 1), ('21442', 2), ('39902', 1), ('2338', 1), ('3148', 1), ('16737', 1), ('11037', 1), ('19572', 1), ('16739', 1), ('35547', 1), ('24806', 1), ('30868', 1), ('2572', 2), ('42911', 1), ('33687', 1), ('1278', 2), ('732', 4), ('597', 1), ('3318', 1), ('34179', 1), ('2471', 2), ('20054', 1), ('10369', 1), ('29762', 1), ('30802', 2), ('29013', 1), ('35034', 1), ('16743', 2), ('41573', 1), ('1152', 1), ('3596', 1), ('18296', 1), ('28421', 2), ('18366', 1), ('1975', 1), ('3073', 1), ('15404', 1), ('1565', 1), ('2715', 1), ('2421', 1), ('1656', 1), ('40732', 1), ('20568', 2), ('25066', 1), ('1153', 5), ('24970', 1), ('2428', 1), ('332', 1), ('14084', 1), ('32253', 1), ('15546', 1), ('14817', 1), ('1920', 1), ('926', 1), ('1715', 2), ('23728', 1), ('3429', 1), ('2535', 1), ('22771', 1), ('29218', 1), ('947', 2), ('2091', 2), ('2033', 2), ('13023', 1), ('23908', 1), ('2500', 1), ('41382', 1), ('33905', 1), ('16464', 1), ('35523', 1), ('8976', 1), ('1759', 1), ('31401', 1), ('22656', 1), ('28417', 2), ('27646', 1), ('18352', 1), ('27282', 1), ('2625', 2), ('30218', 1), ('10944', 1), ('29835', 1), ('24523', 1), ('870', 1), ('26147', 1), ('624', 1), ('18329', 1), ('906', 1), ('508', 1), ('38731', 1), ('1004', 1), ('16893', 1), ('582', 1), ('1027', 1)])\n"
     ]
    }
   ],
   "source": [
    "#here got error\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "for doc in query_corpus:\n",
    "    tf_query = Counter()\n",
    "    for word in re.split(' |\\\\n', doc):\n",
    "        tf_query[word] += 1\n",
    "    del tf_query['']\n",
    "    print(tf_query.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our query vocabulary vector is [36506, 36138, 44870, 27506, 46922, 744, 30055, 2718, 13247, 657, 508, 16868, 1521, 1200, 1999, 9561, 16920, 2875, 24857, 34332, 16276, 10092, 10097, 12928, 597, 12739, 25634, 25312, 38208, 38478, 28516, 21387, 1615, 1788, 47818, 3082, 13894, 2903, 18318, 1259, 30267, 32528, 26416, 1985, 30081, 1715, 21413, 16970, 3043, 32035, 3455, 43965, 24435, 16519, 46265, 31308, 25958, 24040, 13812, 1949, 3153, 2572, 40526, 13674, 25694, 24115, 43444, 24041, 27536, 44782, 31677, 18366, 3025, 24560, 1838, 16972, 3015, 35916, 17428, 13703, 3302, 3429, 1330, 612, 20818, 709, 11437, 9800, 33034, 732, 3068, 2710, 9902, 28193, 22771, 25607, 34802, 13482, 30165, 23840, 2471, 2500, 1004, 2913, 14124, 17231, 19233, 596, 713, 2345, 3413, 17998, 33886, 2997, 37150, 23542, 1944, 35251, 40889, 18412, 29696, 10528, 16271, 8775, 28654, 1190, 653, 1730, 44022]\n",
      "The query is 2280 3068 457 26763 27631 14645 732 4332 2690 2923 20646 38527 27352 \n",
      "612 \n",
      "1407 2986 \n",
      "25607 432 2148 3140 \n",
      "27414 31424 7680 2280 3068 457 24554 30066 855 1715 11629 \n",
      "26763 18571 27631 14645 732 4332 \n",
      "28226 23520 27646 21192 3015 24310 23142 \n",
      "2280 3068 457 1200 \n",
      "596 855 3015 732 1715 2033 \n",
      "27414 2345 2471 699 24310 43624 2878 32518 13629 17554 \n",
      "17030 18380 9561 \n",
      "31765 19448 31359 3012 18828 \n",
      "27646 2345 21192 11738 3015 23600 \n",
      "596 23600 21189 341 12133 3015 1259 18571 27631 3015 14645 \n",
      "7680 2572 8994 2345 21892 3123 2855 \n",
      "8250 32818 699 24310 7971 30165 9608 3015 23142 \n",
      "2280 3068 457 1200 \n",
      "27414 3015 14177 469 1259 732 3237 24583 14177 \n",
      "34043 9079 24310 2938 13109 34474 \n",
      "20818 3015 13629 34445 13910 \n",
      "24310 1408 34770 379 28672 29978 3015 19217 \n",
      "21478 16574 10340 28940 \n",
      "24054 3015 32474 1906 739 34445 2737 \n",
      "9242 2938 1259 10355 3015 15128 \n",
      "1155 34179 13023 \n",
      "8769 17442 2717 24783 14645 19486 \n",
      "3025 3379 39506 \n",
      "27414 2345 1728 34975 662 27647 25995 25944 7570 23390 24310 3015 7971 \n",
      "1027 \n",
      "the tf_query vector for Query 1 is [1, 0, 1, 4, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 6, 1, 2, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 8, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 3, 0, 4, 0, 2, 0, 0, 1, 4, 1, 1, 1, 0, 5, 0, 1, 2, 3, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 1, 1, 4, 0, 0, 1, 1, 2, 1, 0, 2, 2, 1, 0, 2, 0, 2, 1, 1, 1, 0, 6, 1, 0, 1, 2, 0, 0, 2, 1, 1, 1, 0, 0, 0, 12, 0, 1, 1, 0, 2, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 4, 2, 1, 0, 1, 1, 2, 4, 0, 1, 1, 1, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 6, 0, 0, 0, 1, 1, 0, 1, 6, 1, 1, 0, 0, 0, 4, 4, 2, 0, 1, 7, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0]\n",
      "The query is 44394 9715 29223 5580 982 38527 41840 \n",
      "612 \n",
      "1407 2986 21413 7680 44394 \n",
      "1407 9715 29223 1077 36874 \n",
      "1167 3015 1521 35066 1682 16840 28021 1774 3844 2184 \n",
      "3748 985 3981 2446 657 2620 3302 9058 3015 840 27960 \n",
      "739 29223 1155 567 11408 3748 985 3981 2446 2231 32194 18057 \n",
      "44394 596 35066 2913 25607 1200 \n",
      "9242 8805 1259 20417 3015 \n",
      "973 32138 21442 11408 39902 2338 3148 16737 11037 3015 19572 16739 \n",
      "44394 35547 596 24806 21413 30868 2572 42911 25607 33687 1278 \n",
      "732 597 29223 1077 3748 985 3981 2446 657 3302 9058 27960 \n",
      "3318 34179 2471 20054 10369 29762 2913 32194 3015 30802 \n",
      "29013 30802 35034 16743 3015 41573 3981 1152 3596 18296 1200 \n",
      "1521 35066 28421 18366 1975 3073 \n",
      "15404 1565 2715 1155 2421 1656 40732 \n",
      "20568 1259 3748 985 3981 2446 25066 1153 1077 44394 3015 24970 2428 44394 11408 1153 2231 32194 332 14084 3015 20568 \n",
      "1153 1200 \n",
      "9242 32253 1259 1153 1077 3748 985 3981 2446 15546 1278 14817 1920 926 3015 \n",
      "3748 985 3981 2446 \n",
      "1715 \n",
      "612 23728 596 35066 28421 \n",
      "732 1715 3429 2535 22771 \n",
      "44394 29218 947 2091 2033 \n",
      "1407 739 2913 25607 1200 \n",
      "1153 13023 9242 8805 23908 1259 20417 3015 \n",
      "947 2091 2033 2500 1200 \n",
      "41382 \n",
      "44394 33905 16464 35523 8976 1759 3015 2572 31401 22656 3015 28417 \n",
      "9242 28417 2913 27646 21442 18352 27282 \n",
      "21413 2625 2471 30218 10944 2913 32194 29835 24523 \n",
      "732 870 2625 26147 624 18329 \n",
      "906 596 2913 508 732 16743 38731 1004 16893 582 \n",
      "1027 \n",
      "the tf_query vector for Query 2 is [1, 0, 1, 4, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 6, 1, 2, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 8, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 3, 0, 4, 0, 2, 0, 0, 1, 4, 1, 1, 1, 0, 5, 0, 1, 2, 3, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 1, 1, 4, 0, 0, 1, 1, 2, 1, 0, 2, 2, 1, 0, 2, 0, 2, 1, 1, 1, 0, 6, 1, 0, 1, 2, 0, 0, 2, 1, 1, 1, 0, 0, 0, 12, 0, 1, 1, 0, 2, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 4, 2, 1, 0, 1, 1, 2, 4, 0, 1, 1, 1, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 6, 0, 0, 0, 1, 1, 0, 1, 6, 1, 1, 0, 0, 0, 4, 4, 2, 0, 1, 7, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0]\n",
      "All Combined here is our master query term matrix: \n",
      "[[1, 0, 1, 4, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 6, 1, 2, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 8, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 3, 0, 4, 0, 2, 0, 0, 1, 4, 1, 1, 1, 0, 5, 0, 1, 2, 3, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 1, 1, 4, 0, 0, 1, 1, 2, 1, 0, 2, 2, 1, 0, 2, 0, 2, 1, 1, 1, 0, 6, 1, 0, 1, 2, 0, 0, 2, 1, 1, 1, 0, 0, 0, 12, 0, 1, 1, 0, 2, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 4, 2, 1, 0, 1, 1, 2, 4, 0, 1, 1, 1, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 6, 0, 0, 0, 1, 1, 0, 1, 6, 1, 1, 0, 0, 0, 4, 4, 2, 0, 1, 7, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0], [1, 0, 1, 4, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 6, 1, 2, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 8, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 3, 0, 4, 0, 2, 0, 0, 1, 4, 1, 1, 1, 0, 5, 0, 1, 2, 3, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 1, 1, 4, 0, 0, 1, 1, 2, 1, 0, 2, 2, 1, 0, 2, 0, 2, 1, 1, 1, 0, 6, 1, 0, 1, 2, 0, 0, 2, 1, 1, 1, 0, 0, 0, 12, 0, 1, 1, 0, 2, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 4, 2, 1, 0, 1, 1, 2, 4, 0, 1, 1, 1, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 6, 0, 0, 0, 1, 1, 0, 1, 6, 1, 1, 0, 0, 0, 4, 4, 2, 0, 1, 7, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_query = build_lexicon(query_corpus)\n",
    "\n",
    "doc_term_matrix_query = []\n",
    "print('Our query vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "\n",
    "for query in query_corpus:\n",
    "    print('The query is ' + query + '')\n",
    "    tf_query_vector = [tf(word, doc) for word in vocabulary_query]\n",
    "    tf_query_vector_string = ', '.join(format(freq, 'd')for freq in tf_query_vector)\n",
    "    print('the tf_query vector for Query %d is [%s]'%((query_corpus.index(query)+1), tf_query_vector_string))\n",
    "    doc_term_matrix_query.append(tf_query_vector)\n",
    "    \n",
    "print('All Combined here is our master query term matrix: ')\n",
    "print(doc_term_matrix_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regular old query term matrix: \n",
      "[[ 1  0  1  4  0  0  1  1  1  0  0  0  1  0  6  1  2  0  1  0  1  2  0  1\n",
      "   0  0  0  0  0  1  1  1  0  1  1  2  1  5  0  0  0  1  0  0  0  0  0  0\n",
      "   0  0  2  0  0  8  0  1  0  1  1  1  1  1  0  1  1  1  1  1  1  0  1  0\n",
      "   1  0  1  0  0  1  1  2  0  1  3  0  4  0  2  0  0  1  4  1  1  1  0  5\n",
      "   0  1  2  3  0  0  0  1  0  1  1  0  1  1  1  2  2  0  1  2  1  1  4  0\n",
      "   0  1  1  2  1  0  2  2  1  0  2  0  2  1  1  1  0  6  1  0  1  2  0  0\n",
      "   2  1  1  1  0  0  0 12  0  1  1  0  2  0  0  1  2  1  1  1  0  1  0  1\n",
      "   1  0  2  0  4  2  1  0  1  1  2  4  0  1  1  1  3  0  0  3  0  0  0  0\n",
      "   0  0  2  1  1  1  6  0  0  0  1  1  0  1  6  1  1  0  0  0  4  4  2  0\n",
      "   1  7  1  1  1  0  0  1  1  1  1  0  0  1  0  0  0  0  2  0  1  2  0  1\n",
      "   1  1  0]\n",
      " [ 1  0  1  4  0  0  1  1  1  0  0  0  1  0  6  1  2  0  1  0  1  2  0  1\n",
      "   0  0  0  0  0  1  1  1  0  1  1  2  1  5  0  0  0  1  0  0  0  0  0  0\n",
      "   0  0  2  0  0  8  0  1  0  1  1  1  1  1  0  1  1  1  1  1  1  0  1  0\n",
      "   1  0  1  0  0  1  1  2  0  1  3  0  4  0  2  0  0  1  4  1  1  1  0  5\n",
      "   0  1  2  3  0  0  0  1  0  1  1  0  1  1  1  2  2  0  1  2  1  1  4  0\n",
      "   0  1  1  2  1  0  2  2  1  0  2  0  2  1  1  1  0  6  1  0  1  2  0  0\n",
      "   2  1  1  1  0  0  0 12  0  1  1  0  2  0  0  1  2  1  1  1  0  1  0  1\n",
      "   1  0  2  0  4  2  1  0  1  1  2  4  0  1  1  1  3  0  0  3  0  0  0  0\n",
      "   0  0  2  1  1  1  6  0  0  0  1  1  0  1  6  1  1  0  0  0  4  4  2  0\n",
      "   1  7  1  1  1  0  0  1  1  1  1  0  0  1  0  0  0  0  2  0  1  2  0  1\n",
      "   1  1  0]]\n",
      "\n",
      " A query term matrix with row-wise L2 norms of 1: \n",
      "[[ 0.035007    0.          0.035007    0.14002801  0.          0.          0.035007\n",
      "   0.035007    0.035007    0.          0.          0.          0.035007    0.\n",
      "   0.21004201  0.035007    0.070014    0.          0.035007    0.          0.035007\n",
      "   0.070014    0.          0.035007    0.          0.          0.          0.\n",
      "   0.          0.035007    0.035007    0.035007    0.          0.035007\n",
      "   0.035007    0.070014    0.035007    0.17503501  0.          0.          0.\n",
      "   0.035007    0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.070014    0.          0.          0.28005602\n",
      "   0.          0.035007    0.          0.035007    0.035007    0.035007\n",
      "   0.035007    0.035007    0.          0.035007    0.035007    0.035007\n",
      "   0.035007    0.035007    0.035007    0.          0.035007    0.          0.035007\n",
      "   0.          0.035007    0.          0.          0.035007    0.035007\n",
      "   0.070014    0.          0.035007    0.10502101  0.          0.14002801\n",
      "   0.          0.070014    0.          0.          0.035007    0.14002801\n",
      "   0.035007    0.035007    0.035007    0.          0.17503501  0.          0.035007\n",
      "   0.070014    0.10502101  0.          0.          0.          0.035007    0.\n",
      "   0.035007    0.035007    0.          0.035007    0.035007    0.035007\n",
      "   0.070014    0.070014    0.          0.035007    0.070014    0.035007\n",
      "   0.035007    0.14002801  0.          0.          0.035007    0.035007\n",
      "   0.070014    0.035007    0.          0.070014    0.070014    0.035007    0.\n",
      "   0.070014    0.          0.070014    0.035007    0.035007    0.035007    0.\n",
      "   0.21004201  0.035007    0.          0.035007    0.070014    0.          0.\n",
      "   0.070014    0.035007    0.035007    0.035007    0.          0.          0.\n",
      "   0.42008403  0.          0.035007    0.035007    0.          0.070014    0.\n",
      "   0.          0.035007    0.070014    0.035007    0.035007    0.035007    0.\n",
      "   0.035007    0.          0.035007    0.035007    0.          0.070014    0.\n",
      "   0.14002801  0.070014    0.035007    0.          0.035007    0.035007\n",
      "   0.070014    0.14002801  0.          0.035007    0.035007    0.035007\n",
      "   0.10502101  0.          0.          0.10502101  0.          0.          0.\n",
      "   0.          0.          0.          0.070014    0.035007    0.035007\n",
      "   0.035007    0.21004201  0.          0.          0.          0.035007\n",
      "   0.035007    0.          0.035007    0.21004201  0.035007    0.035007    0.\n",
      "   0.          0.          0.14002801  0.14002801  0.070014    0.          0.035007\n",
      "   0.24504901  0.035007    0.035007    0.035007    0.          0.          0.035007\n",
      "   0.035007    0.035007    0.035007    0.          0.          0.035007    0.\n",
      "   0.          0.          0.          0.070014    0.          0.035007\n",
      "   0.070014    0.          0.035007    0.035007    0.035007    0.        ]\n",
      " [ 0.035007    0.          0.035007    0.14002801  0.          0.          0.035007\n",
      "   0.035007    0.035007    0.          0.          0.          0.035007    0.\n",
      "   0.21004201  0.035007    0.070014    0.          0.035007    0.          0.035007\n",
      "   0.070014    0.          0.035007    0.          0.          0.          0.\n",
      "   0.          0.035007    0.035007    0.035007    0.          0.035007\n",
      "   0.035007    0.070014    0.035007    0.17503501  0.          0.          0.\n",
      "   0.035007    0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.070014    0.          0.          0.28005602\n",
      "   0.          0.035007    0.          0.035007    0.035007    0.035007\n",
      "   0.035007    0.035007    0.          0.035007    0.035007    0.035007\n",
      "   0.035007    0.035007    0.035007    0.          0.035007    0.          0.035007\n",
      "   0.          0.035007    0.          0.          0.035007    0.035007\n",
      "   0.070014    0.          0.035007    0.10502101  0.          0.14002801\n",
      "   0.          0.070014    0.          0.          0.035007    0.14002801\n",
      "   0.035007    0.035007    0.035007    0.          0.17503501  0.          0.035007\n",
      "   0.070014    0.10502101  0.          0.          0.          0.035007    0.\n",
      "   0.035007    0.035007    0.          0.035007    0.035007    0.035007\n",
      "   0.070014    0.070014    0.          0.035007    0.070014    0.035007\n",
      "   0.035007    0.14002801  0.          0.          0.035007    0.035007\n",
      "   0.070014    0.035007    0.          0.070014    0.070014    0.035007    0.\n",
      "   0.070014    0.          0.070014    0.035007    0.035007    0.035007    0.\n",
      "   0.21004201  0.035007    0.          0.035007    0.070014    0.          0.\n",
      "   0.070014    0.035007    0.035007    0.035007    0.          0.          0.\n",
      "   0.42008403  0.          0.035007    0.035007    0.          0.070014    0.\n",
      "   0.          0.035007    0.070014    0.035007    0.035007    0.035007    0.\n",
      "   0.035007    0.          0.035007    0.035007    0.          0.070014    0.\n",
      "   0.14002801  0.070014    0.035007    0.          0.035007    0.035007\n",
      "   0.070014    0.14002801  0.          0.035007    0.035007    0.035007\n",
      "   0.10502101  0.          0.          0.10502101  0.          0.          0.\n",
      "   0.          0.          0.          0.070014    0.035007    0.035007\n",
      "   0.035007    0.21004201  0.          0.          0.          0.035007\n",
      "   0.035007    0.          0.035007    0.21004201  0.035007    0.035007    0.\n",
      "   0.          0.          0.14002801  0.14002801  0.070014    0.          0.035007\n",
      "   0.24504901  0.035007    0.035007    0.035007    0.          0.          0.035007\n",
      "   0.035007    0.035007    0.035007    0.          0.          0.035007    0.\n",
      "   0.          0.          0.          0.070014    0.          0.035007\n",
      "   0.070014    0.          0.035007    0.035007    0.035007    0.        ]]\n",
      "here is the length\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_term_matrix_query_l2 = []\n",
    "\n",
    "for vec in doc_term_matrix_query:\n",
    "    doc_term_matrix_query_l2.append(l2_normalizer(vec))\n",
    "    \n",
    "print('A regular old query term matrix: ')\n",
    "print(np.matrix(doc_term_matrix_query))\n",
    "print('\\n A query term matrix with row-wise L2 norms of 1: ')\n",
    "print(np.matrix(doc_term_matrix_query_l2))\n",
    "print('here is the length')\n",
    "print(len(doc_term_matrix_query_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "tfidf_matrix_doc = tfidf_vectorizer.fit_transform(query_corpus)\n",
    "\n",
    "print(tfidf_matrix_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,243) and (129,129) not aligned: 243 (dim 1) != 129 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-848462b2b581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtf_query_vector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_term_matrix_query\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc_term_matrix_query_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix_query_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_idf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#np.dot(np.ones((97,2)), np.ones((2,1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,243) and (129,129) not aligned: 243 (dim 1) != 129 (dim 0)"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_query_tfidf = []\n",
    "\n",
    "for tf_query_vector in doc_term_matrix_query:\n",
    "    doc_term_matrix_query_tfidf.append(np.dot(doc_term_matrix_query_l2, my_idf_matrix))\n",
    "    #np.dot(np.ones((97,2)), np.ones((2,1)))\n",
    "\n",
    "    #doc_term_matrix_query_tfidf.append(doc_term_matrix_query_l2*my_idf_matrix)\n",
    "doc_term_matrix_query_tfidf_l2 = []\n",
    "for tf_query_vector in doc_term_matrix_query_tfidf:\n",
    "    doc_term_matrix_query_tfidf_l2.append(l2_normalizer(tf_query_vector))\n",
    "    \n",
    "print(vocabulary_query)\n",
    "print(np.matrix(doc_term_matrix_query_tfidf_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary_query: {'2280': 39, '3068': 78, '457': 102, '26763': 55, '27631': 61, '14645': 16, '732': 108, '4332': 100, '2690': 56, '2923': 69, '20646': 32, '38527': 97, '27352': 58, '612': 105, '1407': 13, '2986': 71, '25607': 51, '432': 99, '2148': 37, '3140': 81, '27414': 60, '31424': 82, '7680': 111, '24554': 47, '30066': 73, '855': 114, '1715': 20, '11629': 4, '18571': 25, '28226': 64, '23520': 43, '27646': 62, '21192': 35, '3015': 75, '24310': 46, '23142': 40, '1200': 6, '596': 104, '2033': 31, '2345': 42, '2471': 49, '699': 107, '43624': 101, '2878': 67, '32518': 86, '13629': 11, '17554': 23, '17030': 19, '18380': 24, '9561': 119, '31765': 83, '19448': 29, '31359': 80, '3012': 74, '18828': 26, '11738': 5, '23600': 44, '21189': 34, '341': 90, '12133': 7, '1259': 8, '2572': 52, '8994': 116, '21892': 38, '3123': 79, '2855': 65, '8250': 113, '32818': 87, '7971': 112, '30165': 76, '9608': 120, '14177': 15, '469': 103, '3237': 84, '24583': 48, '34043': 89, '9079': 117, '2938': 70, '13109': 10, '34474': 93, '20818': 33, '34445': 92, '13910': 12, '1408': 14, '34770': 94, '379': 96, '28672': 66, '29978': 72, '19217': 28, '21478': 36, '16574': 18, '10340': 1, '28940': 68, '24054': 45, '32474': 85, '1906': 27, '739': 109, '2737': 59, '9242': 118, '10355': 2, '15128': 17, '1155': 3, '34179': 91, '13023': 9, '8769': 115, '17442': 22, '2717': 57, '24783': 50, '19486': 30, '3025': 77, '3379': 88, '39506': 98, '1728': 21, '34975': 95, '662': 106, '27647': 63, '25995': 54, '25944': 53, '7570': 110, '23390': 41, '1027': 0}\n",
      "[[ 0.04593152  0.04593152  0.04593152  0.04593152  0.04593152  0.04593152\n",
      "   0.09186304  0.04593152  0.13779456  0.04593152  0.04593152  0.09186304\n",
      "   0.04593152  0.04593152  0.04593152  0.09186304  0.18372608  0.04593152\n",
      "   0.04593152  0.04593152  0.09186304  0.04593152  0.04593152  0.04593152\n",
      "   0.04593152  0.09186304  0.04593152  0.04593152  0.04593152  0.04593152\n",
      "   0.04593152  0.04593152  0.04593152  0.04593152  0.04593152  0.09186304\n",
      "   0.04593152  0.04593152  0.04593152  0.18372608  0.09186304  0.04593152\n",
      "   0.18372608  0.04593152  0.09186304  0.04593152  0.27558913  0.04593152\n",
      "   0.04593152  0.04593152  0.04593152  0.04593152  0.04593152  0.04593152\n",
      "   0.04593152  0.09186304  0.04593152  0.04593152  0.04593152  0.04593152\n",
      "   0.18372608  0.13779456  0.09186304  0.04593152  0.04593152  0.04593152\n",
      "   0.04593152  0.04593152  0.04593152  0.04593152  0.09186304  0.04593152\n",
      "   0.04593152  0.04593152  0.04593152  0.55117825  0.04593152  0.04593152\n",
      "   0.18372608  0.04593152  0.04593152  0.04593152  0.04593152  0.04593152\n",
      "   0.04593152  0.04593152  0.04593152  0.04593152  0.04593152  0.04593152\n",
      "   0.04593152  0.04593152  0.09186304  0.04593152  0.04593152  0.04593152\n",
      "   0.04593152  0.04593152  0.04593152  0.04593152  0.09186304  0.04593152\n",
      "   0.18372608  0.04593152  0.09186304  0.04593152  0.04593152  0.09186304\n",
      "   0.18372608  0.04593152  0.04593152  0.09186304  0.09186304  0.04593152\n",
      "   0.09186304  0.04593152  0.04593152  0.04593152  0.04593152  0.04593152\n",
      "   0.04593152]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "term_freq_matrix_query = count_vectorizer.fit_transform(query_corpus)\n",
    "print(\"Vocabulary_query:\", count_vectorizer.vocabulary_)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(term_freq_matrix_query)\n",
    "\n",
    "tf_idf_matrix_query = tfidf.transform(term_freq_matrix_query)\n",
    "print(tf_idf_matrix_query.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "term_freq_matrix_query = count_vectorizer.fit_transform(corpusss)\n",
    "print(\"Vocabulary:\", count_vectorizer.vocabulary_)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(term_freq_matrix_query)\n",
    "\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix_query)\n",
    "print(tf_idf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(tf_idf_matrix_query, tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf_matrix_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_term_matrix_tfidf_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
